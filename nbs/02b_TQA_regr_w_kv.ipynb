{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick experiment to see which is better at detecting truthful answers\n",
    "\n",
    "- model outputs\n",
    "- hs\n",
    "- supressed activations (Hypothesis this is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from einops import rearrange, repeat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.data import DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import (\n",
    "    binary_cross_entropy_with_logits as bce_with_logits,\n",
    ")\n",
    "from torch.nn.functional import (\n",
    "    cross_entropy,\n",
    ")\n",
    "from pathlib import Path\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from activation_store.collect import activation_store, default_postprocess_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_mem():\n",
    "    \"\"\"\n",
    "    Clear memory\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    return None\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Qwen/Qwen3-1.7\n",
    "# Qwen/Qwen3-0.6B-FP8\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "batch_size = 6\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "batch_size = 10\n",
    "# model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-3B-Instruct-AWQ\"\n",
    "\n",
    "# model_name = \"AMead10/Llama-3.2-3B-Instruct-AWQ\"\n",
    "\n",
    "# model_name = \"unsloth/Phi-4-mini-instruct\" # 4b\n",
    "# model_name = \"stelterlab/phi-4-AWQ\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16 if ('awq' not in model_name.lower()) else torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",  # flex_attention  flash_attention_2 sdpa eager\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 316\n",
    "max_length = 90\n",
    "split = \"train\"\n",
    "ds1 = load_dataset(\"Yik/truthfulQA-bool\", split=split, keep_in_memory=False)\n",
    "\n",
    "sys_msg = \"\"\"Predict if a statement is true on wikipedia, return 0 for false and 1 for true.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def postprocess_activation_ds_rows(row):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_msg},\n",
    "        {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "        {\"role\": \"assistant\", \"content\": \"The answer is \"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        # add_generation_prompt=True,\n",
    "        continue_final_message=True,\n",
    "        padding_side=\"left\",\n",
    "        truncation_side=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "ds2 = ds1.map(postprocess_activation_ds_rows).with_format(\"torch\")\n",
    "new_cols = list(set(ds2.column_names) - set(ds1.column_names)) + [\"label\"]\n",
    "ds2 = ds2.select_columns(new_cols)\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(ds2['input_ids'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "ds = DataLoader(ds2, batch_size=batch_size, collate_fn=collate_fn)\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose layers to cache\n",
    "# n_layers = model.config.num_hidden_layers\n",
    "# a = int(0.3*n_layers)\n",
    "# b = n_layers-2\n",
    "# layer_groups = {\n",
    "#     'mlp.down_proj': [k for k,v in model.named_modules() if k.endswith('mlp.down_proj')][a:b],\n",
    "#     'self_attn': [k for k,v in model.named_modules() if k.endswith('.self_attn')][a:b],\n",
    "#     'mlp.up_proj': [k for k,v in model.named_modules() if k.endswith('mlp.up_proj')][a:b],\n",
    "# }\n",
    "# layer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose layers to cache\n",
    "n_layers = model.config.num_hidden_layers\n",
    "a = int(0.5*n_layers)\n",
    "b = n_layers-2\n",
    "select = slice(a, b, 3)\n",
    "layer_groups = {\n",
    "    'mlp.down_proj': [k for k,v in model.named_modules() if k.endswith('mlp.down_proj')][select],\n",
    "    'self_attn': [k for k,v in model.named_modules() if k.endswith('.self_attn')][select],\n",
    "    'mlp.up_proj': [k for k,v in model.named_modules() if k.endswith('mlp.up_proj')][select],\n",
    "}\n",
    "layer_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, unicodedata, string\n",
    "from pathlib import Path\n",
    "\n",
    "def sanitize_path(path: Path | str, allow_period: bool = True) -> Path:\n",
    "    \"\"\"\n",
    "    Whitelist only ASCII letters, digits, dash, underscore,\n",
    "    optionally period, and forward‚Äêslash. Replace others with '_'.\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(path))\\\n",
    "                     .encode(\"ascii\", \"ignore\")\\\n",
    "                     .decode()\n",
    "    s = s.replace(os.sep, \"/\")\n",
    "    allowed = set(string.ascii_letters + string.digits + \"_-\")\n",
    "    if allow_period: allowed.add(\".\")\n",
    "    allowed.add(\"/\")\n",
    "    return Path(\"\".join(ch if ch in allowed else \"_\" for ch in s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acts_outfile = Path(f'/tmp/activation_store/ds_at-{model_name.replace(\"/\", \"\")}-truthfulQA-bool-{split}-{len(ds2)}-{max_length}_v2.parquet')\n",
    "acts_outfile = sanitize_path(acts_outfile)\n",
    "acts_outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_tokens(*args, **kwargs):\n",
    "    return default_postprocess_result(*args, **kwargs, last_token=False)\n",
    "\n",
    "\n",
    "f = activation_store(ds, model, layers=layer_groups, postprocess_result=collect_all_tokens, \n",
    "                     outfile=acts_outfile\n",
    "                     )\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO which is better for mem, this or below?\n",
    "ds_a = load_dataset(\"parquet\", split='train', data_files=str(f), keep_in_memory=False).with_format(\"torch\")\n",
    "ds_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO which is better for mem, this or above?\n",
    "# ds_a = Dataset.from_parquet(str(f), split=split, keep_in_memory=False).with_format(\"torch\")\n",
    "# ds_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_groups = [c for c in ds_a.column_names if c.startswith('acts-')]\n",
    "act_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ds_a[0].items():\n",
    "    if hasattr(v, 'shape'):\n",
    "        print(k, v.shape)\n",
    "    else:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity test generate\n",
    "b = next(iter(ds))\n",
    "b = {k: v.to(model.device) for k, v in b.items()}\n",
    "o = model.generate(\n",
    "    inputs=b[\"input_ids\"],\n",
    "    attention_mask=b[\"attention_mask\"],\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "gent = tokenizer.batch_decode(o, skip_special_tokens=False)\n",
    "for g in gent:\n",
    "    print(g)\n",
    "    print(\"---\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get supressed activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_supressed_activations(\n",
    "    hs: Float[Tensor, \"l b t h\"], w_out, w_inv\n",
    ") -> Float[Tensor, \"l b t h\"]:\n",
    "    \"\"\"\n",
    "    Novel experiment: Here we define a transform to isolate supressed activations, where we hypothesis that style/concepts/scratchpads and other internal only representations must be stored.\n",
    "\n",
    "    See the following references for more information:\n",
    "\n",
    "    - https://arxiv.org/pdf/2401.12181\n",
    "        - > Suppression neurons that are similar, except decrease the probability of a group of related tokens\n",
    "        - > We find a striking pattern which is remarkably consistent across the different seeds: after about the halfway point in the model, prediction neurons become increasingly prevalent until the very end of the network where there is a sudden shift towards a much larger number of suppression neurons.\n",
    "\n",
    "    - https://arxiv.org/html/2406.19384\n",
    "        - > Previous work suggests that networks contain ensembles of ‚Äúprediction\" neurons, which act as probability promoters [66, 24, 32] and work in tandem with suppression neurons (Section 5.4).\n",
    "\n",
    "\n",
    "    Output:\n",
    "    - supression amount: This is a tensor of the same shape as the input hs, where the values are the amount of suppression that occured at that layer, and the sign indicates if it was supressed or promoted. How do we calulate this? We project the hs using the output_projection, look at the diff from the last layer, and then project it back using the inverse of the output projection. This gives us the amount of suppression that occured at that layer.\n",
    "    \"\"\"\n",
    "    hs_flat = rearrange(hs[:, :, -1:], \"l b t h -> (l b t) h\")\n",
    "    hs_out_flat = torch.nn.functional.linear(hs_flat, w_out)\n",
    "    hs_out = rearrange(\n",
    "        hs_out_flat, \"(l b t) h -> l b t h\", l=hs.shape[0], b=hs.shape[1], t=1\n",
    "    )\n",
    "    diffs = hs_out[:, :, :].diff(dim=0)\n",
    "    diffs_flat = rearrange(diffs, \"l b t h -> (l b t) h\")\n",
    "    # W_inv = get_cache_inv(w_out)\n",
    "\n",
    "    # get the supression projected back\n",
    "    supr_inv_flat = torch.nn.functional.linear(diffs_flat.to(dtype=w_inv.dtype), w_inv)\n",
    "    supr_amounts = rearrange(\n",
    "        supr_inv_flat, \"(l b t) h -> l b t h\", l=hs.shape[0] - 1, b=hs.shape[1], t=1\n",
    "    ).to(w_out.dtype)\n",
    "\n",
    "    # add on missing first layer\n",
    "    torch.zeros_like(supr_amounts[:1]).to(hs.device)\n",
    "    supr_amounts = torch.cat(\n",
    "        [torch.zeros_like(supr_amounts[:1]).to(hs.device), supr_amounts], dim=0\n",
    "    )\n",
    "    return supr_amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniq_token_ids(tokens):\n",
    "    token_ids = tokenizer(\n",
    "        tokens, add_special_tokens=False, padding=False\n",
    "    ).input_ids\n",
    "    token_ids = torch.tensor(list(set([x[0] for x in token_ids]))).long()\n",
    "    print(\"before\", tokens)\n",
    "    print(\"after\", tokenizer.batch_decode(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "false_tokens = [\"0\", \"0 \", \"0\\n\", \"false\", \"False \"]\n",
    "false_token_ids = get_uniq_token_ids(false_tokens)\n",
    "\n",
    "true_tokens = [\"1\", \"1 \", \"1\\n\", \"true\", \"True \"]\n",
    "true_token_ids = get_uniq_token_ids(true_tokens)\n",
    "\n",
    "print('QC: manually check that these are equivilent (no <end_of_text> or newline)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af953342d91b4ed38e5dbaae9e9a8c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we map to 1) calc supressed activations 2) llm answer (prob of 0 vs prob of 1)\n",
    "\n",
    "Wo = model.get_output_embeddings().weight.detach().clone().cpu()\n",
    "Wo_inv = torch.pinverse(Wo.clone().float())\n",
    "\n",
    "\n",
    "def postprocess_activation_ds_rows(o):\n",
    "    # TODO batch it\n",
    "    \"\"\"Process model outputs\"\"\"\n",
    "\n",
    "    # get llm ans\n",
    "    log_probs = o[\"logits\"][-1].log_softmax(0)\n",
    "    false_log_prob = log_probs.index_select(0, false_token_ids).sum()\n",
    "    true_log_prob = log_probs.index_select(0, true_token_ids).sum()\n",
    "    o[\"llm_ans\"] = torch.stack([false_log_prob, true_log_prob])\n",
    "    o[\"llm_log_prob_true\"] = true_log_prob - false_log_prob\n",
    "\n",
    "    # get supressed activations\n",
    "    hs = o[\"hidden_states\"][None]\n",
    "    hs = rearrange(hs, \"b l t h -> l b t h\")\n",
    "    supr_amounts = get_supressed_activations(hs, Wo.to(hs.dtype), Wo_inv.to(hs.dtype))\n",
    "\n",
    "    # we will only take the last half of layers, and the last token\n",
    "    layer_half = hs.shape[0] // 2\n",
    "    \n",
    "    hs = rearrange(hs, \"l b t h -> b l t h\").squeeze(0)[layer_half:-2]\n",
    "    supr_amounts = rearrange(supr_amounts, \"l b t h -> b l t h\").squeeze(0)[layer_half:-2]\n",
    "\n",
    "    o[\"hidden_states\"] = hs.half()\n",
    "    o[\"supr_amounts\"] = supr_amounts.half()\n",
    "    o['logits'] = o['logits'][-1].half()\n",
    "    return o\n",
    "\n",
    "\n",
    "ds_a2 = ds_a.map(postprocess_activation_ds_rows, writer_batch_size=1, num_proc=None)\n",
    "ds_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wo = Wo_inv = tokenizer = None\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v.shape for k,v in ds_a2[0].items() if isinstance(v, torch.Tensor)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction = 0.2\n",
    "TRAIN_TEST_SPLIT = int(max_length * (1- test_fraction))\n",
    "TRAIN_TEST_SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or Skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.toy import make_regressor\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_linear_prob_on_dataset(\n",
    "    X,\n",
    "    y,\n",
    "    name=\"\",\n",
    "    device: str = \"cuda\",\n",
    "    batch_size=32,\n",
    "):\n",
    "    # flatten\n",
    "    X = X.flatten(1, -1).to(device)\n",
    "    # X = X.view(len(X), -1).to(device)\n",
    "\n",
    "    # norm X\n",
    "    X = ((X - X.mean()) / X.std())\n",
    "    if X.ndim == 1:\n",
    "        X = X.unsqueeze(1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.unsqueeze(1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    # data.shape\n",
    "\n",
    "\n",
    "    lr_model = NeuralNetRegressor(\n",
    "        make_regressor(num_hidden=0, dropout=0, input_units=X.shape[-1]),\n",
    "        lr=0.01,\n",
    "        max_epochs=40,\n",
    "        batch_size=batch_size,\n",
    "        device='cuda',  # uncomment this to train with CUDA\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer__weight_decay=0.001,\n",
    "        verbose=0,\n",
    "    )\n",
    "    # lr_model = Classifier(X.shape[-1], device=device)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr_model.predict_proba(X_test)\n",
    "\n",
    "    score = roc_auc_score(y_test.detach().cpu().numpy(), y_pred)\n",
    "    logger.info(f\"score for probe({name}): {score:.3f} roc auc, n={len(X_test)}. X.shape={X.shape}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jaxtyping\n",
    "# %load_ext jaxtyping\n",
    "# %jaxtyping.typechecker beartype.beartype  # or any other runtime type checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/64947#issuecomment-2304371451\n",
    "from math import ceil, floor\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def torch_quantile(\n",
    "    input: torch.Tensor,\n",
    "    q: float | torch.Tensor,\n",
    "    dim: int | None = None,\n",
    "    keepdim: bool = False,\n",
    "    *,\n",
    "    interpolation: str = \"nearest\",\n",
    "    out: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Better torch.quantile for one SCALAR quantile.\n",
    "\n",
    "    Using torch.kthvalue. Better than torch.quantile because:\n",
    "        - No 2**24 input size limit (pytorch/issues/67592),\n",
    "        - Much faster, at least on big input sizes.\n",
    "\n",
    "    Arguments:\n",
    "        input (torch.Tensor): See torch.quantile.\n",
    "        q (float): See torch.quantile. Supports only scalar input\n",
    "            currently.\n",
    "        dim (int | None): See torch.quantile.\n",
    "        keepdim (bool): See torch.quantile. Supports only False\n",
    "            currently.\n",
    "        interpolation: {\"nearest\", \"lower\", \"higher\"}\n",
    "            See torch.quantile.\n",
    "        out (torch.Tensor | None): See torch.quantile. Supports only\n",
    "            None currently.\n",
    "    \"\"\"\n",
    "    # Sanitization: q\n",
    "    try:\n",
    "        q = float(q)\n",
    "        assert 0 <= q <= 1\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Only scalar input 0<=q<=1 is currently supported (got {q})!\")\n",
    "\n",
    "    # Sanitization: dim\n",
    "    # Because one cannot pass  `dim=None` to `squeeze()` or `kthvalue()`\n",
    "    if dim_was_none := dim is None:\n",
    "        dim = 0\n",
    "        input = input.reshape((-1,) + (1,) * (input.ndim - 1))\n",
    "\n",
    "    # Sanitization: inteporlation\n",
    "    if interpolation == \"nearest\":\n",
    "        inter = round\n",
    "    elif interpolation == \"lower\":\n",
    "        inter = floor\n",
    "    elif interpolation == \"higher\":\n",
    "        inter = ceil\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Supported interpolations currently are {'nearest', 'lower', 'higher'} \"\n",
    "            f\"(got '{interpolation}')!\"\n",
    "        )\n",
    "\n",
    "    # Sanitization: out\n",
    "    if out is not None:\n",
    "        raise ValueError(f\"Only None value is currently supported for out (got {out})!\")\n",
    "\n",
    "    # Logic\n",
    "    k = inter(q * (input.shape[dim] - 1)) + 1\n",
    "    out = torch.kthvalue(input, k, dim, keepdim=True, out=out)[0]\n",
    "\n",
    "    # Rectification: keepdim\n",
    "    if keepdim:\n",
    "        return out\n",
    "    if dim_was_none:\n",
    "        return out.squeeze()\n",
    "    else:\n",
    "        return out.squeeze(dim)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "FiltIn = Float[Tensor, 'B L T H']\n",
    "FiltOut = Tuple[FiltIn, FiltIn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_hs_sup(o: Dataset, thresh: float = 1.0e-2) -> FiltOut:\n",
    "    \"\"\"\n",
    "    Calc supressed activations for a certain threshold\n",
    "    \"\"\"\n",
    "    supr_amounts = o[\"supr_amounts\"]\n",
    "    hs = o[\"hidden_states\"] # [b l h]\n",
    "    if thresh > 0:\n",
    "        supressed_mask = (supr_amounts > thresh).to(hs.dtype)# [b l h]\n",
    "    else:\n",
    "        supressed_mask = (supr_amounts < thresh).to(hs.dtype)\n",
    "    return hs * supressed_mask\n",
    "    # return {\n",
    "    #     f'supressed_hs_{thresh}':hs * supressed_mask\n",
    "    # }\n",
    "\n",
    "# for eps in [-10, -5, -1, -0.5, -0.1, -0.01, -0, 0, 0.01, 0.1, 0.5, 1, 10]:\n",
    "#     ds_a2 = ds_a2.map(calc_hs_sup, fn_kwargs={'eps': eps}, writer_batch_size=1, num_proc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def magnitude_filtered_post_softmax(x: FiltIn, quantile=0.9) -> FiltOut:\n",
    "    \"\"\"Filter out tokens with abnormally high post-softmax values\"\"\"\n",
    "    # Apply softmax to get attention-like weights\n",
    "    weights = torch.softmax(x, dim=-1).max(dim=-1, keepdim=True).values\n",
    "    \n",
    "    # Create mask for tokens below threshold\n",
    "    # FIXME should be independant of batch\n",
    "    threshold = torch_quantile(weights, quantile)\n",
    "    mask = (weights <= threshold)\n",
    "    \n",
    "    # # Ensure we don't filter everything out\n",
    "    if mask.sum() == 0:\n",
    "        logger.warning(f\"All tokens filtered out threshold={threshold}.\")\n",
    "    #     # Keep all but the highest attention token\n",
    "    #     _, max_idx = weights.max(dim=2)\n",
    "    #     mask = torch.ones_like(weights, dtype=torch.bool)\n",
    "    #     mask[max_idx] = False\n",
    "    \n",
    "    return x * mask, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_filtered(x: FiltIn, quantile=.9) -> FiltOut:\n",
    "    \"\"\"Filter out tokens with attention weights above a quantile threshold\"\"\"\n",
    "    weights = torch.log_softmax(x, dim=-1)#.max(dim=-1, keepdim=True).values\n",
    "    # can't use max as there are too many attention sinks\n",
    "    # print(weights)\n",
    "    # FIXME should be independant of batch\n",
    "    threshold = torch_quantile(weights, quantile)\n",
    "    mask = weights <= threshold\n",
    "    # print(weights.shape, mask.shape, mask.float().mean(), threshold)\n",
    "    if mask.sum() == 0:\n",
    "        logger.warning(f\"All tokens filtered out quantile={quantile}.\")\n",
    "    return x * mask, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = ds_a2['hidden_states']\n",
    "# print(X.shape)\n",
    "# X2, mask = quantile_filtered(X)\n",
    "# mask.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy_guided_filter(x: FiltIn, quantile=0.9, normalize=True) -> FiltOut:\n",
    "    \"\"\"Filter tokens with entropy-based threshold using mean/std statistics\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape [tokens, hidden_dim]\n",
    "        z_threshold: How many standard deviations from mean to use as threshold\n",
    "        \n",
    "    Returns:\n",
    "        Filtered tensor with high-attention tokens removed\n",
    "    \"\"\"\n",
    "    # Get attention-like weights\n",
    "    p = torch.softmax(x, dim=-1)\n",
    "    entropy_per_token = -(p * torch.log(p + 1e-8)).sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    # optionally normalize by max entropy (log of feature dimension)\n",
    "    if normalize:\n",
    "        denom = math.log(p.shape[-1])\n",
    "        entropy_per_token = entropy_per_token / denom\n",
    "    \n",
    "    # Get statistics across tokens\n",
    "    # FIXME should be independant of batch\n",
    "    threshold = torch_quantile(entropy_per_token, quantile)\n",
    "    \n",
    "    # Create mask for tokens below threshold\n",
    "    mask = entropy_per_token <= threshold\n",
    "    \n",
    "    # Ensure we don't filter everything\n",
    "    if mask.sum() == 0:\n",
    "        logger.warning(f\"All tokens filtered out z_threshold={quantile}.\")\n",
    "        # mask = torch.ones_like(weights, dtype=torch.bool)\n",
    "        # mask[entropy_per_token.argmax()] = False\n",
    "    \n",
    "    return x * mask, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = ds_a2['hidden_states']\n",
    "# X2, mask = entropy_guided_filter(X)\n",
    "# X.shape, X2.shape, X.norm(), X2.norm(), mask.shape, mask.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new reduction functions that filter out potential attention sinks\n",
    "def filter_special_positions(x: FiltIn, positions_to_exclude=[0, -1]) -> FiltIn:\n",
    "    \"\"\"Filter out specific positions like first (BOS) and last token\"\"\"\n",
    "    mask = torch.ones(x.shape, dtype=torch.bool, device=x.device)\n",
    "    for pos in positions_to_exclude:\n",
    "        if pos < 0:\n",
    "            actual_pos = x.shape[0] + pos\n",
    "        else:\n",
    "            actual_pos = pos\n",
    "        if 0 <= actual_pos < x.shape[0]:\n",
    "            mask[actual_pos] = False\n",
    "    return x * mask, mask\n",
    "\n",
    "def filter_high_magnitude(x: FiltIn, threshold_factor=2.0) -> FiltIn:\n",
    "    \"\"\"Filter out tokens with abnormally high magnitude (potential attention sinks)\"\"\"\n",
    "    magnitudes = torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "    # FIXME should be independant of batch\n",
    "    mean_mag = magnitudes.mean()\n",
    "    std_mag = magnitudes.std()\n",
    "    threshold = mean_mag + threshold_factor * std_mag\n",
    "    mask = magnitudes <= threshold\n",
    "    if mask.sum() > 0:  # Ensure we don't filter everything\n",
    "        return x * mask, mask\n",
    "    else:\n",
    "        # Fallback: keep all but the highest magnitude\n",
    "        _, sorted_indices = torch.sort(magnitudes, descending=True)\n",
    "        mask = torch.ones_like(magnitudes, dtype=torch.bool)\n",
    "        mask[sorted_indices[0]] = False\n",
    "        return x * mask, mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = ds_a2['hidden_states']\n",
    "# X2 = filter_high_magnitude(X)\n",
    "# X.shape, X2.shape, X.norm(), X2.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define permutations of token_reductions, datasets, filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = [\"supr_amounts\", \"hidden_states\",] + act_groups\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# plain cols\n",
    "for c in X_cols:\n",
    "    datasets[c] = lambda ds_a2: ds_a2[c]\n",
    "\n",
    "# differen't suppressed activations\n",
    "for eps in [-5, -1, -0.5, -0.1, -0.01, -0, 0, 0.01, 0.1, 0.5, 1, 5]:\n",
    "    datasets[f'supressed_hs_{eps}'] = lambda ds_a2: transform_hs_sup(ds_a2, eps)\n",
    "\n",
    "\n",
    "# filters/transformers which we apply to all\n",
    "filters = {\n",
    "    \"special\": filter_special_positions,\n",
    "    \"magnitude\": filter_high_magnitude,\n",
    "    \"entropy\": entropy_guided_filter,\n",
    "    \"quantile\": quantile_filtered,\n",
    "}\n",
    "# also diff magnitude filters\n",
    "for eps in [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n",
    "    filters[f'magnitude_{eps}'] = lambda x: filter_high_magnitude(x, eps)\n",
    "\n",
    "# # 2. token aggregators\n",
    "# token_level_funcs = {\n",
    "#     \"min:\": lambda x: x.min(0)[0],\n",
    "#     \"max\": lambda x: x.max(0)[0],\n",
    "#     \"mean\": lambda x: x.mean(0),\n",
    "#     \"sum\": lambda x: x.sum(0),\n",
    "#     \"first\": lambda x: x[0],\n",
    "#     \"last\": lambda x: x[-1],\n",
    "#     # \"none\": lambda x: x,\n",
    "#     \"std\": lambda x: x.std(0),\n",
    "# }\n",
    "# unit test agg\n",
    "token_level_funcs = {\n",
    "    \"min:\": lambda x: x.min(2)[0],\n",
    "    \"max\": lambda x: x.max(2)[0],\n",
    "    \"mean\": lambda x: x.mean(2),\n",
    "    \"sum\": lambda x: x.sum(2),\n",
    "    \"first\": lambda x: x[:, :, 0],\n",
    "    \"last\": lambda x: x[:, :, -1],\n",
    "    # \"none\": lambda x: x,\n",
    "    \"flatten\": lambda x: x.flatten(2),\n",
    "    \"std\": lambda x: x.std(2),\n",
    "}\n",
    "\n",
    "# now get and shuffle all perms\n",
    "perms = []\n",
    "for k, _ in datasets.items():\n",
    "    for k2, _ in filters.items():\n",
    "        for k3, _ in token_level_funcs.items():\n",
    "            perms.append((k, k2, k3))\n",
    "\n",
    "\n",
    "perms = list(perms)\n",
    "random.Random(42).shuffle(perms)\n",
    "perms[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unit test filter\n",
    "# X0 = ds_a2['hidden_states']\n",
    "# for k,v in filters.items():\n",
    "#     X0_norm = X0.norm()\n",
    "#     X2, mask = v(X0) # bad\n",
    "#     print(f\"filter={k}\\n\\tmask.mean()={mask.float().mean():2.2%} \\n\\tmask={mask.shape}, \\n\\toutput={X0.shape}->{X2.shape}, \\n\\tnorm={X0_norm}->{X2.norm():2.6f}={(X0_norm-X2.norm())/X0_norm:.6%}\")\n",
    "#     assert mask.float().mean() < 1.0\n",
    "#     assert mask.float().mean() > 0.0\n",
    "#     assert X2.shape == X0.shape\n",
    "#     assert X2.norm() < X0_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path('../outputs')\n",
    "output_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, (ds_key, filter_key, token_key) in tqdm(enumerate(perms), total=len(perms)):\n",
    "\n",
    "    name = f\"{ds_key}_{filter_key}_{token_key}\"\n",
    "    res_f = output_path / f\"{acts_outfile.stem}_{ds_key}_{filter_key}_{token_key}.json\"\n",
    "    res_f = sanitize_path(res_f)\n",
    "    if res_f.exists():\n",
    "        d = json.load(res_f.open())\n",
    "        logger.info(f'Already processed {res_f}, skipping {d[\"score\"]:2.2f}')\n",
    "        results.append((name, d[\"score\"]))\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # TODO I would also like to cache the results\n",
    "        logger.info(f\"Processing {ds_key}, {filter_key}, {token_key}\")\n",
    "        ds = datasets[ds_key]\n",
    "        filter_func = filters[filter_key]\n",
    "        token_func = token_level_funcs[token_key]\n",
    "\n",
    "        # get the data\n",
    "        X = ds(ds_a2)\n",
    "        logger.info(f\"ds.shape: {X.shape}\")\n",
    "        X, mask = filter_func(X)\n",
    "        logger.info(f\"Xfilt.shape: {X.shape}\")\n",
    "        X = token_func(X)\n",
    "        logger.info(f\"Xtkn.shape: {X.shape}\")\n",
    "\n",
    "        # train the model\n",
    "        y = ds_a2[\"label\"].to('cuda').float()\n",
    "        score = train_linear_prob_on_dataset(X, y, name=f\"{ds_key}_{filter_key}_{token_key}\")\n",
    "        results.append((ds_key, filter_key, token_key, score))\n",
    "\n",
    "        res = {\n",
    "            \"ds_key\": ds_key,\n",
    "            \"filter_key\": filter_key,\n",
    "            \"token_key\": token_key,\n",
    "            \"score\": score,\n",
    "        }\n",
    "        json.dump(res, open(res_f, \"w\"))\n",
    "        results.append((name, score))\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(f\"KeyboardInterrupt, stopping {ds_key}, {filter_key}, {token_key}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {ds_key}, {filter_key}, {token_key}: {e}\")\n",
    "        raise\n",
    "        continue\n",
    "\n",
    "    clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add baselines (llm_ans_ logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"logits\"\n",
    "# X = ds_a2['logits']\n",
    "# score = train_linear_prob_on_dataset(X, name)\n",
    "# results.append((name, score))\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = ds_a2['llm_ans'].exp()\n",
    "# X = X[:, 1] / (X[:, 0] + X[:, 1])\n",
    "X = X.argmax(1)\n",
    "y = ds_a2['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_fraction, shuffle=False)\n",
    "\n",
    "score = roc_auc_score(y_test, X_test).item()\n",
    "if score<0.5:\n",
    "    score = 1-score\n",
    "results.append(('llm_ans', score))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.sigmoid(ds_a2['llm_log_prob_true']/10) # would be better to calibrate or logreg\n",
    "y = ds_a2['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_fraction, shuffle=False)\n",
    "\n",
    "score = roc_auc_score(y_test, X_test).item()\n",
    "results.append(('llm_log_prob_true', score))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"name\", \"auroc\"]).sort_values(\n",
    "    \"auroc\", ascending=False\n",
    ")\n",
    "print(model_name, \"Top results\")\n",
    "print(df.head(20).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'] = df['name'].apply(lambda x: x.split()[0])\n",
    "# FIXME this is not keeping name and auroc paired\n",
    "# df['reduction'] = df['name'].apply(lambda x: x.split()[-1])\n",
    "df2 = df.groupby('data').apply(lambda g: g.sort_values(\"auroc\", ascending=False).iloc[0], include_groups=False).sort_values(\"auroc\", ascending=False)\n",
    "print('top reduction for each data type')\n",
    "print(df2.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "c = ['llm_ans', 'llm_log_prob_true', 'hidden_states',  'supressed_hs'] + act_groups\n",
    "df3 = df2.T[c].rename(columns={\n",
    "    'llm_ans': 'LLM Answer',\n",
    "    'llm_log_prob_true': 'LLM Probability',\n",
    "    'hidden_states': 'Hidden States',\n",
    "    'acts': 'Activations: up_proj',\n",
    "    # 'logits': 'Logits',\n",
    "    'supressed_hs': 'Supressed Hidden States',\n",
    "}).T.sort_values(\"auroc\", ascending=False)\n",
    "# df3.plot.barh()\n",
    "sns.barplot(data=df3, x='auroc', y=df3.index)\n",
    "plt.legend().remove()\n",
    "plt.xlabel(\"Linear probe AUROC\")\n",
    "plt.title(f\"TruthfulQA Binary with {model_name}\")\n",
    "plt.xlim(0.5, None)\n",
    "f = Path('../figs/').joinpath(f\"truthfulqa_{model_name.replace('/', '_')}.png\")\n",
    "plt.savefig(str(f), bbox_inches='tight')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
